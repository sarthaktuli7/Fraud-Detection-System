{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5eb4de5-7559-4c9f-9d72-2d05da1a357d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üõ°Ô∏è FRAUD DETECTION MODEL TRAINING\n",
      "============================================================\n",
      "üìÖ Started at: 2026-01-08 14:54:47\n",
      "\n",
      "============================================================\n",
      "üìä STEP 1: Generating Synthetic Dataset\n",
      "============================================================\n",
      "‚úÖ Dataset created: 50,000 transactions\n",
      "‚úÖ Fraud rate: 3.50%\n",
      "‚úÖ Features: 23 columns\n",
      "\n",
      "============================================================\n",
      "üîß STEP 2: Feature Engineering\n",
      "============================================================\n",
      "‚úÖ Original features: 23\n",
      "‚úÖ Engineered features: 29\n",
      "\n",
      "============================================================\n",
      "‚öñÔ∏è STEP 3: Preparing Training Data\n",
      "============================================================\n",
      "‚úÖ Training samples (before SMOTE): 40,000\n",
      "‚úÖ Training samples (after SMOTE): 77,200\n",
      "‚úÖ Test samples: 10,000\n",
      "\n",
      "============================================================\n",
      "ü§ñ STEP 4: Training Models\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£ Training Random Forest...\n",
      "   ‚úÖ ROC-AUC: 0.9998\n",
      "\n",
      "2Ô∏è‚É£ Training XGBoost...\n",
      "   ‚úÖ ROC-AUC: 0.9998\n",
      "\n",
      "3Ô∏è‚É£ Training Logistic Regression...\n",
      "   ‚úÖ ROC-AUC: 0.9999\n",
      "\n",
      "============================================================\n",
      "üéØ STEP 5: Creating Ensemble Model\n",
      "============================================================\n",
      "\n",
      "üìä ENSEMBLE PERFORMANCE:\n",
      "   ‚Ä¢ ROC-AUC:   0.9999\n",
      "   ‚Ä¢ F1-Score:  0.9559\n",
      "\n",
      "üìã Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       1.00      1.00      1.00      9650\n",
      "       Fraud       0.92      0.99      0.96       350\n",
      "\n",
      "    accuracy                           1.00     10000\n",
      "   macro avg       0.96      0.99      0.98     10000\n",
      "weighted avg       1.00      1.00      1.00     10000\n",
      "\n",
      "\n",
      "============================================================\n",
      "üíæ STEP 6: Saving Models\n",
      "============================================================\n",
      "‚úÖ Saved: models/feature_engineering_pipeline.pkl\n",
      "‚úÖ Saved: models/fraud_detection_ensemble.pkl\n",
      "‚úÖ Saved: Individual model files\n",
      "\n",
      "============================================================\n",
      "üîç STEP 7: Verifying Model Loading\n",
      "============================================================\n",
      "‚úÖ Test prediction successful!\n",
      "   ‚Ä¢ Fraud probability: 0.0007\n",
      "   ‚Ä¢ Risk level: LOW\n",
      "\n",
      "============================================================\n",
      "üéâ TRAINING COMPLETE!\n",
      "============================================================\n",
      "\n",
      "‚úÖ All models trained and saved successfully!\n",
      "\n",
      "üìÅ Files created in 'models/' folder:\n",
      "   ‚Ä¢ feature_engineering_pipeline.pkl\n",
      "   ‚Ä¢ fraud_detection_ensemble.pkl\n",
      "   ‚Ä¢ random_forest_model.pkl\n",
      "   ‚Ä¢ xgboost_model.pkl\n",
      "   ‚Ä¢ logistic_regression_model.pkl\n",
      "   ‚Ä¢ scaler.pkl\n",
      "\n",
      "üöÄ NEXT STEPS:\n",
      "   1. Start the API:\n",
      "      uvicorn src.api.fraud_api:app --reload --port 8000\n",
      "   \n",
      "   2. Start the Dashboard:\n",
      "      streamlit run src/monitoring/dashboard.py\n",
      "   \n",
      "   3. Test the API:\n",
      "      Open http://localhost:8000/docs\n",
      "\n",
      "üìÖ Completed at: 2026-01-08 14:54:57\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMPLETE FRAUD DETECTION TRAINING SCRIPT\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üõ°Ô∏è FRAUD DETECTION MODEL TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üìÖ Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: Generate Synthetic Dataset\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä STEP 1: Generating Synthetic Dataset\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def create_fraud_dataset(n_samples=50000, fraud_rate=0.035):\n",
    "    \"\"\"Generate synthetic credit card fraud dataset\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    n_fraud = int(n_samples * fraud_rate)\n",
    "    n_normal = n_samples - n_fraud\n",
    "    \n",
    "    # Normal transactions\n",
    "    normal_data = {'Time': np.random.uniform(0, 172800, n_normal),\n",
    "                   'Amount': np.abs(np.random.exponential(50, n_normal))}\n",
    "    for i in range(1, 21):\n",
    "        normal_data[f'V{i}'] = np.random.normal(0, 1, n_normal)\n",
    "    normal_data['Class'] = 0\n",
    "    df_normal = pd.DataFrame(normal_data)\n",
    "    \n",
    "    # Fraudulent transactions\n",
    "    fraud_data = {'Time': np.random.uniform(0, 172800, n_fraud),\n",
    "                  'Amount': np.abs(np.random.exponential(150, n_fraud))}\n",
    "    for i in range(1, 21):\n",
    "        if i in [1, 3, 4, 7, 10, 12, 14, 17]:\n",
    "            fraud_data[f'V{i}'] = np.random.normal(-2, 1.5, n_fraud)\n",
    "        else:\n",
    "            fraud_data[f'V{i}'] = np.random.normal(0.5, 1.2, n_fraud)\n",
    "    fraud_data['Class'] = 1\n",
    "    df_fraud = pd.DataFrame(fraud_data)\n",
    "    \n",
    "    df = pd.concat([df_normal, df_fraud], ignore_index=True)\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "df = create_fraud_dataset(n_samples=50000)\n",
    "print(f\"‚úÖ Dataset created: {len(df):,} transactions\")\n",
    "print(f\"‚úÖ Fraud rate: {df['Class'].mean()*100:.2f}%\")\n",
    "print(f\"‚úÖ Features: {df.shape[1]} columns\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2: Feature Engineering\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîß STEP 2: Feature Engineering\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "class SimpleFeatureEngineering:\n",
    "    \"\"\"Simple feature engineering pipeline that works with the API\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_names = None\n",
    "        self.fitted = False\n",
    "    \n",
    "    def fit(self, df):\n",
    "        \"\"\"Fit the feature engineering pipeline\"\"\"\n",
    "        df_transformed = self._add_features(df.copy())\n",
    "        X = df_transformed.drop('Class', axis=1, errors='ignore')\n",
    "        self.feature_names = list(X.columns)\n",
    "        self.scaler.fit(X)\n",
    "        self.fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        \"\"\"Transform data using fitted pipeline\"\"\"\n",
    "        df_transformed = self._add_features(df.copy())\n",
    "        \n",
    "        # Ensure all expected columns exist\n",
    "        for col in self.feature_names:\n",
    "            if col not in df_transformed.columns:\n",
    "                df_transformed[col] = 0\n",
    "        \n",
    "        # Select only the columns we trained on (in correct order)\n",
    "        X = df_transformed[self.feature_names]\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        return pd.DataFrame(X_scaled, columns=self.feature_names)\n",
    "    \n",
    "    def fit_transform(self, df):\n",
    "        \"\"\"Fit and transform in one step\"\"\"\n",
    "        self.fit(df)\n",
    "        return self.transform(df)\n",
    "    \n",
    "    def _add_features(self, df):\n",
    "        \"\"\"Add engineered features\"\"\"\n",
    "        # Time-based features\n",
    "        df['Hour'] = (df['Time'] % 86400) / 3600\n",
    "        df['Is_Night'] = ((df['Hour'] >= 22) | (df['Hour'] <= 5)).astype(int)\n",
    "        \n",
    "        # Amount-based features\n",
    "        df['Amount_Log'] = np.log1p(df['Amount'])\n",
    "        \n",
    "        # V-feature interactions\n",
    "        df['V1_V3_product'] = df['V1'] * df['V3']\n",
    "        df['V4_V12_product'] = df['V4'] * df['V12']\n",
    "        \n",
    "        # Statistical aggregations\n",
    "        v_cols = [f'V{i}' for i in range(1, 21)]\n",
    "        df['V_mean'] = df[v_cols].mean(axis=1)\n",
    "        df['V_std'] = df[v_cols].std(axis=1)\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Create and fit feature engineering pipeline\n",
    "fe_pipeline = SimpleFeatureEngineering()\n",
    "df_transformed = fe_pipeline._add_features(df.copy())\n",
    "\n",
    "X = df_transformed.drop('Class', axis=1)\n",
    "y = df_transformed['Class']\n",
    "\n",
    "print(f\"‚úÖ Original features: {df.shape[1]}\")\n",
    "print(f\"‚úÖ Engineered features: {X.shape[1]}\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 3: Prepare Training Data\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚öñÔ∏è STEP 3: Preparing Training Data\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"‚úÖ Training samples (before SMOTE): {len(y_train):,}\")\n",
    "print(f\"‚úÖ Training samples (after SMOTE): {len(y_train_balanced):,}\")\n",
    "print(f\"‚úÖ Test samples: {len(y_test):,}\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 4: Train Models\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ü§ñ STEP 4: Training Models\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Random Forest\n",
    "print(\"\\n1Ô∏è‚É£ Training Random Forest...\")\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100, max_depth=10, class_weight='balanced',\n",
    "    random_state=42, n_jobs=-1\n",
    ")\n",
    "rf_model.fit(X_train_balanced, y_train_balanced)\n",
    "rf_proba = rf_model.predict_proba(X_test_scaled)[:, 1]\n",
    "print(f\"   ‚úÖ ROC-AUC: {roc_auc_score(y_test, rf_proba):.4f}\")\n",
    "\n",
    "# XGBoost\n",
    "print(\"\\n2Ô∏è‚É£ Training XGBoost...\")\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=100, max_depth=6, learning_rate=0.1,\n",
    "    scale_pos_weight=len(y_train[y_train==0])/len(y_train[y_train==1]),\n",
    "    random_state=42, eval_metric='auc', use_label_encoder=False\n",
    ")\n",
    "xgb_model.fit(X_train_balanced, y_train_balanced)\n",
    "xgb_proba = xgb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "print(f\"   ‚úÖ ROC-AUC: {roc_auc_score(y_test, xgb_proba):.4f}\")\n",
    "\n",
    "# Logistic Regression\n",
    "print(\"\\n3Ô∏è‚É£ Training Logistic Regression...\")\n",
    "lr_model = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42)\n",
    "lr_model.fit(X_train_balanced, y_train_balanced)\n",
    "lr_proba = lr_model.predict_proba(X_test_scaled)[:, 1]\n",
    "print(f\"   ‚úÖ ROC-AUC: {roc_auc_score(y_test, lr_proba):.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 5: Create Ensemble Model\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ STEP 5: Creating Ensemble Model\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "class EnsembleModel:\n",
    "    \"\"\"Ensemble model that combines RF, XGB, and LR predictions\"\"\"\n",
    "    \n",
    "    def __init__(self, rf_model, xgb_model, lr_model, scaler, feature_names, weights=None):\n",
    "        self.rf_model = rf_model\n",
    "        self.xgb_model = xgb_model\n",
    "        self.lr_model = lr_model\n",
    "        self.scaler = scaler\n",
    "        self.feature_names = feature_names\n",
    "        self.weights = weights or {'rf': 0.3, 'xgb': 0.5, 'lr': 0.2}\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict fraud probability using weighted ensemble\"\"\"\n",
    "        # Ensure X is a DataFrame with correct columns\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            # Add missing columns with zeros\n",
    "            for col in self.feature_names:\n",
    "                if col not in X.columns:\n",
    "                    X[col] = 0\n",
    "            X = X[self.feature_names]\n",
    "            X_scaled = self.scaler.transform(X)\n",
    "        else:\n",
    "            X_scaled = X\n",
    "        \n",
    "        # Get predictions from each model\n",
    "        rf_proba = self.rf_model.predict_proba(X_scaled)[:, 1]\n",
    "        xgb_proba = self.xgb_model.predict_proba(X_scaled)[:, 1]\n",
    "        lr_proba = self.lr_model.predict_proba(X_scaled)[:, 1]\n",
    "        \n",
    "        # Weighted average\n",
    "        ensemble_proba = (\n",
    "            self.weights['rf'] * rf_proba +\n",
    "            self.weights['xgb'] * xgb_proba +\n",
    "            self.weights['lr'] * lr_proba\n",
    "        )\n",
    "        \n",
    "        # Return in sklearn format (n_samples, 2)\n",
    "        return np.column_stack([1 - ensemble_proba, ensemble_proba])\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        proba = self.predict_proba(X)[:, 1]\n",
    "        return (proba >= 0.5).astype(int)\n",
    "\n",
    "# Create ensemble\n",
    "ensemble_model = EnsembleModel(\n",
    "    rf_model=rf_model,\n",
    "    xgb_model=xgb_model,\n",
    "    lr_model=lr_model,\n",
    "    scaler=scaler,\n",
    "    feature_names=list(X.columns)\n",
    ")\n",
    "\n",
    "# Test ensemble\n",
    "ensemble_proba = ensemble_model.predict_proba(X_test_scaled)[:, 1]\n",
    "ensemble_pred = (ensemble_proba >= 0.5).astype(int)\n",
    "\n",
    "print(f\"\\nüìä ENSEMBLE PERFORMANCE:\")\n",
    "print(f\"   ‚Ä¢ ROC-AUC:   {roc_auc_score(y_test, ensemble_proba):.4f}\")\n",
    "print(f\"   ‚Ä¢ F1-Score:  {f1_score(y_test, ensemble_pred):.4f}\")\n",
    "print(\"\\nüìã Classification Report:\")\n",
    "print(classification_report(y_test, ensemble_pred, target_names=['Normal', 'Fraud']))\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 6: Save Models\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üíæ STEP 6: Saving Models\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create models directory\n",
    "models_dir = r'C:\\Users\\sarthak\\OneDrive\\Desktop\\fraud-detection-system-main\\models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Save feature engineering pipeline (the API expects this)\n",
    "fe_pipeline.scaler = scaler\n",
    "fe_pipeline.feature_names = list(X.columns)\n",
    "fe_pipeline.fitted = True\n",
    "joblib.dump(fe_pipeline, os.path.join(models_dir, 'feature_engineering_pipeline.pkl'))\n",
    "print(f\"‚úÖ Saved: models/feature_engineering_pipeline.pkl\")\n",
    "\n",
    "# Save ensemble model (the API expects this)\n",
    "joblib.dump(ensemble_model, os.path.join(models_dir, 'fraud_detection_ensemble.pkl'))\n",
    "print(f\"‚úÖ Saved: models/fraud_detection_ensemble.pkl\")\n",
    "\n",
    "# Save individual models (optional, for backup)\n",
    "joblib.dump(rf_model, os.path.join(models_dir, 'random_forest_model.pkl'))\n",
    "joblib.dump(xgb_model, os.path.join(models_dir, 'xgboost_model.pkl'))\n",
    "joblib.dump(lr_model, os.path.join(models_dir, 'logistic_regression_model.pkl'))\n",
    "joblib.dump(scaler, os.path.join(models_dir, 'scaler.pkl'))\n",
    "print(f\"‚úÖ Saved: Individual model files\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 7: Verify Models Load Correctly\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîç STEP 7: Verifying Model Loading\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test loading\n",
    "loaded_fe = joblib.load(os.path.join(models_dir, 'feature_engineering_pipeline.pkl'))\n",
    "loaded_ensemble = joblib.load(os.path.join(models_dir, 'fraud_detection_ensemble.pkl'))\n",
    "\n",
    "# Test prediction\n",
    "test_transaction = pd.DataFrame([{\n",
    "    'Time': 12345, 'Amount': 149.62,\n",
    "    'V1': -1.36, 'V2': -0.07, 'V3': 2.54, 'V4': 1.38, 'V5': -0.34,\n",
    "    'V6': 0.46, 'V7': 0.24, 'V8': 0.10, 'V9': 0.36, 'V10': 0.09,\n",
    "    'V11': -0.55, 'V12': -0.62, 'V13': -0.99, 'V14': -0.31, 'V15': 1.47,\n",
    "    'V16': -0.47, 'V17': 0.21, 'V18': 0.03, 'V19': 0.40, 'V20': 0.25\n",
    "}])\n",
    "\n",
    "# Transform and predict\n",
    "test_transformed = loaded_fe.transform(test_transaction)\n",
    "test_proba = loaded_ensemble.predict_proba(test_transformed)[0, 1]\n",
    "print(f\"‚úÖ Test prediction successful!\")\n",
    "print(f\"   ‚Ä¢ Fraud probability: {test_proba:.4f}\")\n",
    "print(f\"   ‚Ä¢ Risk level: {'HIGH' if test_proba > 0.7 else 'MEDIUM' if test_proba > 0.3 else 'LOW'}\")\n",
    "\n",
    "# =============================================================================\n",
    "# DONE!\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\"\"\n",
    "‚úÖ All models trained and saved successfully!\n",
    "\n",
    "üìÅ Files created in 'models/' folder:\n",
    "   ‚Ä¢ feature_engineering_pipeline.pkl\n",
    "   ‚Ä¢ fraud_detection_ensemble.pkl\n",
    "   ‚Ä¢ random_forest_model.pkl\n",
    "   ‚Ä¢ xgboost_model.pkl\n",
    "   ‚Ä¢ logistic_regression_model.pkl\n",
    "   ‚Ä¢ scaler.pkl\n",
    "\n",
    "üöÄ NEXT STEPS:\n",
    "   1. Start the API:\n",
    "      uvicorn src.api.fraud_api:app --reload --port 8000\n",
    "   \n",
    "   2. Start the Dashboard:\n",
    "      streamlit run src/monitoring/dashboard.py\n",
    "   \n",
    "   3. Test the API:\n",
    "      Open http://localhost:8000/docs\n",
    "\n",
    "üìÖ Completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cbb135-feb6-4766-81a9-30748e1167d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
